#!/usr/bin/env python3
"""
Webç‰ˆæœ¬çš„CustomCrawler - åŸºäºåŸç‰ˆcustom_crawler_for_specific_task.py
ç§»é™¤GUIå¼¹çª—ï¼Œæ”¹ä¸ºWebçŠ¶æ€å›è°ƒæœºåˆ¶
"""

import time
import csv
import random
import re
import json
import os
import threading
from datetime import datetime
from playwright.sync_api import sync_playwright
import logging
from fake_useragent import UserAgent
from .anti_detection_config import AntiDetectionConfig

class WebCustomCrawler:
    """Webç‰ˆæœ¬çš„å®šåˆ¶åŒ–çˆ¬è™« - å»é™¤GUIï¼Œæ·»åŠ çŠ¶æ€å›è°ƒ"""
    
    def __init__(self, cookie_string, status_callback=None):
        """
        åˆå§‹åŒ–Webçˆ¬è™«
        Args:
            cookie_string: Cookieå­—ç¬¦ä¸²
            status_callback: çŠ¶æ€å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–°Webç•Œé¢çŠ¶æ€
        """
        self.cookie_string = cookie_string
        self.status_callback = status_callback
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.captcha_count = 0
        self.skipped_pages = 0
        self.daily_request_count = 0
        self.max_daily_requests = 200
        self.consecutive_failures = 0
        self.max_consecutive_failures = 5
        self.page_refresh_count = 0
        self.ua_change_count = 0
        
        # åˆå§‹åŒ–User-Agentç”Ÿæˆå™¨
        self.ua = UserAgent()
        
        # è®¾ç½®è¯¦ç»†æ—¥å¿—
        self._setup_detailed_logging()
        
        # åŸå¸‚é…ç½®
        self.cities = {
            'é•¿æ²™å¸‚': 'changsha',
            'æ·±åœ³å¸‚': 'shenzhen',
            'è‹å·å¸‚': 'suzhou',
            'å—å®å¸‚': 'nanning',
            'ä¸Šæµ·å¸‚': 'shanghai',
            'å¹¿å·å¸‚': 'guangzhou',
            'æ­å·å¸‚': 'hangzhou',
            'å¦é—¨å¸‚': 'xiamen',
            'æ­¦æ±‰å¸‚': 'wuhan',
            'è¥¿å®‰å¸‚': 'xian',
            'åŒ—äº¬å¸‚':'beijing'
        }
        
        # å“ç±»é…ç½®ï¼ˆå·²éªŒè¯çš„å“ç±»IDï¼‰
        self.categories = {
            'çƒ¤è‚‰': 'g34303',
            'é¢åŒ…è›‹ç³•ç”œå“': "g117",
            'æ—¥å¼æ–™ç†': 'g113',
            'å·èœ': 'g102',
            'æ°´æœç”Ÿé²œ': 'g2714',
            'æ±Ÿæµ™èœ': 'g101',
            'å°åƒå¿«é¤': 'g112',
            'ç²¤èœ': 'g103',
            'ç«é”…': 'g110',
            'çƒ§çƒ¤çƒ¤ä¸²': 'g508',
            'å°é¾™è™¾': 'g219',  # ä¿®æ­£IDä»g1204åˆ°g219
            'å’–å•¡': 'g132',
            'é¥®å“': 'g34236',
            'åœ°æ–¹èœç³»': 'g34351',
            # ä»é¡µé¢HTMLä¸­æ–°å¢çš„å“ç±»
            'è‡ªåŠ©é¤': 'g111',
            'ç‰¹è‰²èœ': 'g34284',
            'é£Ÿå“æ»‹è¡¥': 'g33759',
            'è¥¿é¤': 'g116',
            'éŸ©å¼æ–™ç†': 'g114',
            'é¢é¦†': 'g215',
            'æ¹˜èœ': 'g104',
            'é™•èœ': 'g34234',
            'é±¼é²œæµ·é²œ': 'g251',
            'ä¸œåŒ—èœ': 'g106',
            'æ–°ç–†èœ': 'g3243',
            'å†œå®¶èœ': 'g25474',
            'åŒ—äº¬èœ': 'g311',
            'å®¶å¸¸èœ': 'g1783',
            'ç§æˆ¿èœ': 'g1338',
            'èºè›³ç²‰': 'g32725',
            'åˆ›æ„èœ': 'g250',
            'ä¸œå—äºšèœ': 'g115',
            'ä¸­ä¸œèœ': 'g234',
            'éæ´²èœ': 'g2797',
            'å…¶ä»–ç¾é£Ÿ': 'g118'
        }
        
        # æ ¸å¿ƒå­—æ®µ
        self.core_fields = [
            'city',
            'primary_category',
            'secondary_category',
            'shop_name',
            'avg_price',
            'review_count',  # æ–°å¢è¯„ä»·æ•°é‡å­—æ®µ
            'rating'         # æ–°å¢è¯„åˆ†ç­‰çº§å­—æ®µ
        ]
        
        self.all_data = []

    def _setup_detailed_logging(self):
        """è®¾ç½®è¯¦ç»†æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºä»»åŠ¡ä¸“ç”¨logger
        self.logger = logging.getLogger(f'web_crawler_{id(self)}')
        if not self.logger.handlers:
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
            
            # æ–‡ä»¶å¤„ç†å™¨
            log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data', 'logs')
            os.makedirs(log_dir, exist_ok=True)
            log_file = os.path.join(log_dir, f'crawler_{datetime.now().strftime("%Y%m%d")}.log')
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

    def _update_status(self, message, progress=None, status_type='info', detailed=False):
        """æ›´æ–°çŠ¶æ€åˆ°Webç•Œé¢ï¼ŒåŒæ—¶è®°å½•è¯¦ç»†æ—¥å¿—"""
        # è®°å½•è¯¦ç»†æ—¥å¿—
        log_level = {
            'info': logging.INFO,
            'warning': logging.WARNING,
            'error': logging.ERROR,
            'success': logging.INFO
        }.get(status_type, logging.INFO)
        
        self.logger.log(log_level, f"[{status_type.upper()}] {message}")
        
        # å‘é€åˆ°Webç•Œé¢
        if self.status_callback:
            self.status_callback({
                'message': message,
                'progress': progress,
                'type': status_type,
                'timestamp': datetime.now().isoformat(),
                'stats': {
                    'captcha_count': self.captcha_count,
                    'skipped_pages': self.skipped_pages,
                    'page_refresh_count': self.page_refresh_count,
                    'ua_change_count': self.ua_change_count
                }
            })

    def get_random_user_agent(self):
        """éšæœºç”ŸæˆUser-Agent"""
        try:
            browsers = ['chrome', 'firefox', 'edge']
            browser = random.choice(browsers)
            
            if browser == 'chrome':
                user_agent = self.ua.chrome
            elif browser == 'firefox':
                user_agent = self.ua.firefox
            else:  # edge
                user_agent = self.ua.edge
            
            return user_agent
            
        except Exception as e:
            # å¤‡ç”¨User-Agentåˆ—è¡¨
            fallback_uas = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
            ]
            return random.choice(fallback_uas)

    def get_random_viewport(self):
        """éšæœºç”Ÿæˆè§†çª—å¤§å°"""
        viewports = [
            {'width': 1920, 'height': 1080},
            {'width': 1366, 'height': 768},
            {'width': 1536, 'height': 864},
            {'width': 1440, 'height': 900},
            {'width': 1600, 'height': 900},
            {'width': 1280, 'height': 720}
        ]
        return random.choice(viewports)

    def get_browser_fingerprint_script(self):
        """ç”Ÿæˆç®€åŒ–ç‰ˆæµè§ˆå™¨æŒ‡çº¹ä¼ªè£…è„šæœ¬ - ç§»é™¤å¤æ‚åŠŸèƒ½æé«˜ç¨³å®šæ€§"""
        language = random.choice(['zh-CN', 'en-US'])
        platform = random.choice(['Win32', 'Win64'])
        
        script = f"""
        // åŸºæœ¬çš„webdriveræ ‡è¯†ç§»é™¤
        Object.defineProperty(navigator, 'webdriver', {{
            get: () => undefined
        }});
        
        // åˆ é™¤è‡ªåŠ¨åŒ–ç›¸å…³å±æ€§
        try {{
            if (window.cdc_adoQpoasnfa76pfcZLmcfl_Array) {{
                delete window.cdc_adoQpoasnfa76pfcZLmcfl_Array;
            }}
            if (window.cdc_adoQpoasnfa76pfcZLmcfl_Promise) {{
                delete window.cdc_adoQpoasnfa76pfcZLmcfl_Promise;
            }}
            if (window.cdc_adoQpoasnfa76pfcZLmcfl_Symbol) {{
                delete window.cdc_adoQpoasnfa76pfcZLmcfl_Symbol;
            }}
        }} catch (e) {{
            // å¿½ç•¥é”™è¯¯
        }}
        
        // åŸºæœ¬çš„navigatorä¿¡æ¯ - ä½¿ç”¨å®‰å…¨çš„æ–¹å¼
        try {{
            Object.defineProperty(navigator, 'language', {{
                get: () => '{language}'
            }});
            Object.defineProperty(navigator, 'platform', {{
                get: () => '{platform}'
            }});
        }} catch (e) {{
            // å¿½ç•¥é”™è¯¯ï¼Œä¿æŒç¨³å®šæ€§
        }}
        
        console.log('[FINGERPRINT] ç®€åŒ–ç‰ˆæµè§ˆå™¨æŒ‡çº¹ä¼ªè£…å·²åŠ è½½');
        """
        return script

    def create_browser_context(self, playwright_instance):
        """åˆ›å»ºå¸¦æœ‰éšæœºæŒ‡çº¹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        try:
            self.logger.info("[BROWSER] ğŸš€ å¼€å§‹åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡...")
            user_agent = self.get_random_user_agent()
            viewport = self.get_random_viewport()
            
            self.logger.info(f"[BROWSER] ğŸ”§ User-Agent: {user_agent[:50]}...")
            self.logger.info(f"[BROWSER] ğŸ“ è§†å£å¤§å°: {viewport['width']}x{viewport['height']}")
            
            browser = playwright_instance.chromium.launch(
                headless=False,
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--disable-hang-monitor',
                    '--disable-prompt-on-repost',
                    '--disable-default-apps',
                    '--disable-logging',
                    '--lang=zh-CN',
                    '--accept-lang=zh-CN,zh;q=0.9,en;q=0.8',
                    f'--user-agent={user_agent}'
                ]
            )
            
            context = browser.new_context(
                user_agent=user_agent,
                viewport=viewport,
                locale='zh-CN',
                timezone_id='Asia/Shanghai',
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': str(random.choice([0, 1])),
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )
            
            self.logger.info("[BROWSER] âœ… æµè§ˆå™¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ")
            return browser, context
            
        except Exception as e:
            self.logger.error(f"[BROWSER] âŒ åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            self.logger.error(f"[BROWSER] ğŸ” å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            self._update_status(f"åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}", status_type='error')
            raise

    def parse_cookies(self):
        """è§£æCookieå­—ç¬¦ä¸²ï¼ˆå¢å¼ºç‰ˆè®¾å¤‡éš”ç¦»ï¼‰"""
        cookies = []
        for cookie_pair in self.cookie_string.split('; '):
            if '=' in cookie_pair:
                name, value = cookie_pair.split('=', 1)
                # è¿‡æ»¤æ‰å¯èƒ½åŒ…å«è®¾å¤‡æŒ‡çº¹çš„Cookie
                if name.strip().lower() not in ['device_id', 'fingerprint', 'client_id', 'session_id']:
                    cookies.append({
                        'name': name.strip(),
                        'value': value.strip(),
                        'domain': '.dianping.com',
                        'path': '/'
                    })
        return cookies
    
    def clear_browser_data(self, context):
        """æ¸…ç†æµè§ˆå™¨æ•°æ®ä»¥é¿å…è®¾å¤‡å…³è”"""
        try:
            # æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®
            context.clear_cookies()
            
            # æ‰§è¡Œæ¸…ç†è„šæœ¬
            page = context.new_page()
            page.evaluate("""
                // æ¸…ç†æœ¬åœ°å­˜å‚¨
                if (window.localStorage) {
                    window.localStorage.clear();
                }
                
                // æ¸…ç†ä¼šè¯å­˜å‚¨
                if (window.sessionStorage) {
                    window.sessionStorage.clear();
                }
                
                // æ¸…ç†IndexedDB
                if (window.indexedDB) {
                    window.indexedDB.databases().then(databases => {
                        databases.forEach(db => {
                            window.indexedDB.deleteDatabase(db.name);
                        });
                    }).catch(() => {});
                }
                
                // æ¸…ç†ç¼“å­˜
                if ('caches' in window) {
                    caches.keys().then(names => {
                        names.forEach(name => {
                            caches.delete(name);
                        });
                    }).catch(() => {});
                }
            """)
            page.close()
            self.logger.info("[PRIVACY] âœ… æµè§ˆå™¨æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.warning(f"[PRIVACY] âš ï¸ æ¸…ç†æµè§ˆå™¨æ•°æ®æ—¶å‡ºç°è­¦å‘Š: {e}")

    def extract_shop_data(self, page, city_name, category_name):
        """æå–å•†é“ºæ•°æ®"""
        try:
            page.wait_for_load_state('networkidle', timeout=20000)
            
            if 'login' in page.url.lower():
                self._update_status("Cookieå¤±æ•ˆï¼Œè¢«é‡å®šå‘åˆ°ç™»å½•é¡µé¢", status_type='error')
                return []
            
            content = page.content()
            shops = []

            # ä½¿ç”¨æ›´é€šç”¨çš„æ–¹å¼åŒ¹é…å•†é“ºä¿¡æ¯å—
            # å°è¯•å¤šç§å¯èƒ½çš„HTMLç»“æ„
            shop_block_patterns = [
                r'<li class="">(.*?)</li>',  # åŸå§‹æ¨¡å¼
                r'<li[^>]*>(.*?)</li>',      # ä»»ä½•liæ ‡ç­¾
                r'<div[^>]*class="[^"]*shop-wrap[^"]*"[^>]*>(.*?)</div>',  # å•†é“ºåŒ…è£…div
                r'<div[^>]*class="[^"]*shop[^"]*"[^>]*>(.*?)</div>',       # å•†é“ºdiv
                r'<div[^>]*data-click-name="[^"]*shop[^"]*"[^>]*>(.*?)</div>'  # å¸¦æ•°æ®å±æ€§çš„div
            ]

            shop_blocks = []
            for pattern in shop_block_patterns:
                blocks = re.findall(pattern, content, re.DOTALL)
                if blocks:
                    shop_blocks = blocks
                    self.logger.info(f"[PARSE] ä½¿ç”¨æ¨¡å¼åŒ¹é…åˆ° {len(blocks)} ä¸ªå•†é“ºå—")
                    break

            if not shop_blocks:
                self.logger.warning("[PARSE] æœªæ‰¾åˆ°å•†é“ºä¿¡æ¯å—ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢æå–")
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å—ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢å†…å®¹ä¸­ç›´æ¥æå–
                shop_blocks = [content]

            for block in shop_blocks:
                try:
                    # å•†é“ºåç§°æå– - ä½¿ç”¨å¤šç§æ¨¡å¼
                    name_patterns = [
                        r'<h4>([^<]+)</h4>',                    # åŸå§‹æ¨¡å¼
                        r'<h3[^>]*>([^<]+)</h3>',              # h3æ ‡ç­¾
                        r'<h2[^>]*>([^<]+)</h2>',              # h2æ ‡ç­¾
                        r'class="[^"]*shopname[^"]*"[^>]*>([^<]+)',  # å•†åº—åç§°ç±»
                        r'class="[^"]*shop-name[^"]*"[^>]*>([^<]+)', # å•†åº—åç§°ç±»å˜ä½“
                        r'data-click-name="[^"]*"[^>]*>([^<]+)</a>',  # å¸¦æ•°æ®å±æ€§çš„é“¾æ¥
                    ]

                    name_match = None
                    shop_name = ""
                    for pattern in name_patterns:
                        name_match = re.search(pattern, block)
                        if name_match:
                            shop_name = name_match.group(1).strip()
                            if shop_name and len(shop_name) > 1:  # ç¡®ä¿å•†é“ºåç§°æœ‰æ„ä¹‰
                                break

                    if not shop_name or len(shop_name) < 2:
                        continue

                    # ä»·æ ¼æå– - ä½¿ç”¨æ›´å¤šæ¨¡å¼
                    avg_price = ""
                    price_patterns = [
                        r'<b>ï¿¥(\d+)</b>',
                        r'ï¿¥(\d+)',
                        r'äººå‡[ï¼š:]?\s*ï¿¥?(\d+)',
                        r'å¹³å‡[ï¼š:]?\s*ï¿¥?(\d+)',
                        r'price[^>]*>ï¿¥?(\d+)',
                        r'avgprice[^>]*>ï¿¥?(\d+)'
                    ]
                    for pattern in price_patterns:
                        price_match = re.search(pattern, block)
                        if price_match:
                            avg_price = price_match.group(1)
                            break

                    # æå–è¯„ä»·æ•°é‡ - ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…
                    review_count = ""
                    review_patterns = [
                        r'<b>(\d+)</b>\s*æ¡è¯„ä»·',          # ä¸»è¦æ¨¡å¼ï¼š<b>9766</b>æ¡è¯„ä»·
                        r'<b>(\d+)</b>\s*æ¡ç‚¹è¯„',          # <b>æ•°å­—</b>æ¡ç‚¹è¯„
                        r'review-num[^>]*>.*?<b>(\d+)</b>', # review-numç±»ä¸­çš„<b>æ ‡ç­¾
                        r'class="review-num"[^>]*>.*?<b>(\d+)</b>', # å®Œæ•´çš„review-numç±»åŒ¹é…
                        r'(\d+)\s*æ¡è¯„ä»·',                 # ç®€å•æ¨¡å¼ï¼šæ•°å­—æ¡è¯„ä»·
                        r'(\d+)\s*æ¡ç‚¹è¯„',                 # ç®€å•æ¨¡å¼ï¼šæ•°å­—æ¡ç‚¹è¯„
                        r'(\d+)\s*è¯„ä»·',                   # æ•°å­—è¯„ä»·
                        r'(\d+)\s*ç‚¹è¯„',                   # æ•°å­—ç‚¹è¯„
                        r'è¯„ä»·\s*(\d+)',                   # è¯„ä»·æ•°å­—
                        r'ç‚¹è¯„\s*(\d+)',                   # ç‚¹è¯„æ•°å­—
                        r'<span[^>]*>(\d+)</span>\s*æ¡',   # spanæ ‡ç­¾ä¸­çš„æ•°å­—
                        r'>(\d+)</\w+>\s*æ¡'               # ä»»ä½•æ ‡ç­¾ä¸­çš„æ•°å­—åè·Ÿ"æ¡"
                    ]
                    for pattern in review_patterns:
                        review_match = re.search(pattern, block, re.DOTALL)
                        if review_match:
                            review_count = review_match.group(1)
                            self.logger.debug(f"[DATA] è¯„ä»·æ•°åŒ¹é…æˆåŠŸ: {review_count} (æ¨¡å¼: {pattern[:30]}...)")
                            break

                    # æå–è¯„åˆ†ç­‰çº§ - ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…æ˜Ÿçº§
                    rating = ""
                    star_patterns = [
                        r'star\s+star_(\d+)\s+star_sml',       # åŸå§‹æ¨¡å¼
                        r'class="[^"]*star[^"]*star_(\d+)[^"]*"', # CSSç±»ä¸­çš„star_æ•°å­—
                        r'star_(\d+)',                          # ç®€å•star_æ•°å­—
                        r'rating-(\d+)',                        # rating-æ•°å­—
                        r'score-(\d+)',                         # score-æ•°å­—
                        r'class="[^"]*star[^"]*(\d{2})[^"]*"',  # ç±»åä¸­çš„ä¸¤ä½æ•°å­—
                        r'star(\d{2})',                         # staråè·Ÿä¸¤ä½æ•°å­—
                        r'<span[^>]*class="[^"]*star[^"]*"[^>]*>.*?(\d\.\d)</span>', # spanä¸­çš„å°æ•°è¯„åˆ†
                        r'>(\d\.\d)</',                         # ä»»ä½•æ ‡ç­¾ä¸­çš„å°æ•°è¯„åˆ†
                        r'å¹³å‡åˆ†[ï¼š:]?\s*(\d+\.?\d*)',           # å¹³å‡åˆ†æ–‡å­—
                        r'è¯„åˆ†[ï¼š:]?\s*(\d+\.?\d*)',             # è¯„åˆ†æ–‡å­—
                    ]
                    for pattern in star_patterns:
                        star_match = re.search(pattern, block, re.DOTALL)
                        if star_match:
                            star_value = star_match.group(1)
                            try:
                                # å¤„ç†ä¸åŒæ ¼å¼ï¼š45->4.5, 4->4.0, 4.5->4.5
                                if '.' in star_value:
                                    # å·²ç»æ˜¯å°æ•°æ ¼å¼
                                    rating = star_value
                                elif len(star_value) == 2:
                                    # ä¸¤ä½æ•°å­—ï¼Œå¦‚45->4.5
                                    rating = str(int(star_value) / 10)
                                elif len(star_value) == 1:
                                    # ä¸€ä½æ•°å­—ï¼Œå¦‚4->4.0
                                    rating = star_value + ".0"
                                else:
                                    rating = str(float(star_value))

                                # éªŒè¯è¯„åˆ†èŒƒå›´ï¼ˆ1-5åˆ†ï¼‰
                                if float(rating) > 5.0:
                                    rating = ""
                                    continue

                                self.logger.debug(f"[DATA] è¯„åˆ†åŒ¹é…æˆåŠŸ: {rating} (åŸå€¼: {star_value}, æ¨¡å¼: {pattern[:30]}...)")
                            except:
                                rating = ""
                                continue
                            if rating:
                                break

                    # è°ƒè¯•æ—¥å¿— - è®°å½•æå–ç»“æœ
                    if review_count or rating:
                        self.logger.info(f"[DATA] å•†å®¶: {shop_name[:10]}... è¯„ä»·æ•°: {review_count} è¯„åˆ†: {rating}")
                    else:
                        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¾“å‡ºéƒ¨åˆ†HTMLç”¨äºè°ƒè¯•
                        debug_block = block[:500] if len(block) > 500 else block
                        self.logger.debug(f"[DATA] å•†å®¶: {shop_name[:10]}... æœªæ‰¾åˆ°è¯„ä»·æ•°å’Œè¯„åˆ†")
                        self.logger.debug(f"[DEBUG] HTMLç‰‡æ®µ: {debug_block[:200]}...")

                    shop = {
                        'city': city_name,
                        'primary_category': 'ç¾é£Ÿ',
                        'secondary_category': category_name,
                        'shop_name': shop_name,
                        'avg_price': avg_price,
                        'review_count': review_count,  # æ–°å¢è¯„ä»·æ•°é‡
                        'rating': rating  # æ–°å¢è¯„åˆ†ç­‰çº§
                    }
                    
                    shops.append(shop)
                    
                except Exception as e:
                    continue
            
            return shops
            
        except Exception as e:
            self._update_status(f"æ•°æ®æå–å¤±è´¥: {e}", status_type='error')
            return []

    def detect_captcha(self, page):
        """æ£€æµ‹éªŒè¯ç """
        try:
            captcha_selectors = [
                '.captcha',
                '#captcha',
                '[class*="verify"]',
                '[id*="verify"]',
                '.verification',
                '[class*="captcha"]'
            ]

            for selector in captcha_selectors:
                element = page.query_selector(selector)
                if element and element.is_visible():
                    return f"æ£€æµ‹åˆ°å¯è§éªŒè¯ç å…ƒç´ : {selector}"

            title = page.title().lower()
            if ('éªŒè¯ä¸­å¿ƒ' in title or 'verification center' in title or
                'captcha' in title or 'äººæœºéªŒè¯' in title):
                page_content = page.content().lower()
                if ('éªŒè¯ç ' in page_content or 'captcha' in page_content or
                    'äººæœºéªŒè¯' in page_content or 'verification' in page_content):
                    return f"é¡µé¢æ ‡é¢˜å’Œå†…å®¹ç¡®è®¤ä¸ºéªŒè¯é¡µé¢: {title}"

            return None

        except Exception as e:
            return None

    def simulate_user_behavior(self, page):
        """ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¢åŠ ç¨³å®šæ€§å’Œé”™è¯¯å¤„ç†"""
        try:
            self._update_status("å¼€å§‹ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ...")
            
            # æ£€æŸ¥é¡µé¢çŠ¶æ€
            if not self._check_page_status(page):
                self.logger.warning("[BEHAVIOR] âš ï¸ é¡µé¢çŠ¶æ€å¼‚å¸¸ï¼Œè·³è¿‡ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ")
                return
            
            # 1. åˆå§‹ç­‰å¾… - ç¼©çŸ­æ—¶é—´
            initial_wait = random.uniform(1, 3)
            time.sleep(initial_wait)
            
            # 2. ç®€åŒ–çš„é¼ æ ‡ç§»åŠ¨ - å‡å°‘æ¬¡æ•°å’ŒèŒƒå›´
            try:
                mouse_moves = random.randint(1, 3)  # ä»2-8å‡å°‘åˆ°1-3æ¬¡
                for i in range(mouse_moves):
                    if not self._check_page_status(page):
                        break
                    
                    # æ›´å®‰å…¨çš„åæ ‡èŒƒå›´
                    x = random.randint(400, 1000)
                    y = random.randint(200, 600)
                    
                    self._safe_mouse_move(page, x, y)
                    time.sleep(random.uniform(0.5, 1.5))  # ç¼©çŸ­å»¶è¿Ÿ
            except Exception as e:
                self.logger.warning(f"[BEHAVIOR] é¼ æ ‡ç§»åŠ¨å¼‚å¸¸: {e}")
            
            # 3. ç§»é™¤éšæœºç‚¹å‡» - è¿™æ˜¯å¯¼è‡´é—®é¢˜çš„ä¸»è¦åŸå› 
            # åŸæ¥çš„éšæœºç‚¹å‡»ä»£ç è¢«å®Œå…¨ç§»é™¤
            
            # 4. æ›´å®‰å…¨çš„æ»šåŠ¨æ“ä½œ
            try:
                self._safe_scroll_behavior(page)
            except Exception as e:
                self.logger.warning(f"[BEHAVIOR] æ»šåŠ¨æ“ä½œå¼‚å¸¸: {e}")
            
            # 5. ç§»é™¤é¡µé¢é‡è½½ - è¿™æ˜¯å¯¼è‡´å´©æºƒçš„ä¸»è¦åŸå› 
            # åŸæ¥çš„é¡µé¢é‡è½½ä»£ç è¢«ç§»é™¤
            
            # 6. æœ€ç»ˆç­‰å¾… - ç¼©çŸ­æ—¶é—´
            try:
                stay_pattern = random.choice(['short', 'medium'])  # ç§»é™¤longé€‰é¡¹
                if stay_pattern == 'short':
                    final_wait = random.uniform(1, 3)  # ä»2-5ç¼©çŸ­
                else:  # medium
                    final_wait = random.uniform(2, 4)  # ä»4-8ç¼©çŸ­
                time.sleep(final_wait)
            except Exception as e:
                self.logger.warning(f"[BEHAVIOR] æœ€ç»ˆç­‰å¾…å¼‚å¸¸: {e}")
            
            self.logger.info("[BEHAVIOR] âœ… ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"[BEHAVIOR] âŒ ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿå¤±è´¥: {e}")
            time.sleep(random.uniform(2, 4))  # ç¼©çŸ­é”™è¯¯æ¢å¤æ—¶é—´
    
    def _check_page_status(self, page):
        """æ£€æŸ¥é¡µé¢çŠ¶æ€æ˜¯å¦æ­£å¸¸"""
        try:
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´æ£€æŸ¥é¡µé¢çŠ¶æ€
            page.evaluate("document.readyState", timeout=5000)
            return True
        except Exception:
            return False
    
    def _safe_mouse_move(self, page, x, y):
        """å®‰å…¨çš„é¼ æ ‡ç§»åŠ¨æ“ä½œ"""
        try:
            page.mouse.move(x, y)
        except Exception as e:
            self.logger.debug(f"[BEHAVIOR] é¼ æ ‡ç§»åŠ¨å¤±è´¥: {e}")
    
    def _safe_scroll_behavior(self, page):
        """å®‰å…¨çš„æ»šåŠ¨è¡Œä¸º"""
        try:
            scroll_type = random.choice(['simple', 'reading'])  # ç®€åŒ–é€‰é¡¹
            
            if scroll_type == 'simple':
                # ç®€å•æ»šåŠ¨ - åªæ»šåŠ¨2-3æ¬¡
                for i in range(random.randint(2, 3)):
                    if not self._check_page_status(page):
                        break
                    
                    scroll_pos = random.uniform(0.2, 0.8)
                    self._safe_evaluate(
                        page, 
                        f"window.scrollTo(0, document.body.scrollHeight * {scroll_pos})"
                    )
                    time.sleep(random.uniform(1, 2))  # ç¼©çŸ­å»¶è¿Ÿ
            else:  # reading
                # é˜…è¯»å¼æ»šåŠ¨
                positions = [0.3, 0.6]
                for pos in positions:
                    if not self._check_page_status(page):
                        break
                    
                    self._safe_evaluate(
                        page,
                        f"window.scrollTo(0, document.body.scrollHeight * {pos})"
                    )
                    time.sleep(random.uniform(1, 2))
        except Exception as e:
            self.logger.warning(f"[BEHAVIOR] æ»šåŠ¨è¡Œä¸ºå¼‚å¸¸: {e}")
    
    def _safe_evaluate(self, page, script, timeout=5000):
        """å®‰å…¨çš„é¡µé¢è„šæœ¬æ‰§è¡Œ"""
        try:
            return page.evaluate(script, timeout=timeout)
        except Exception as e:
            self.logger.debug(f"[BEHAVIOR] è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def _is_browser_alive(self, page):
        """æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦è¿˜æ´»ç€"""
        try:
            if page is None or page.is_closed():
                return False
            # å°è¯•è·å–é¡µé¢URLï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ£€æŸ¥
            page.url
            return True
        except Exception as e:
            self.logger.warning(f"[BROWSER] æµè§ˆå™¨çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _handle_browser_crash(self, error_msg):
        """å¤„ç†æµè§ˆå™¨å´©æºƒ"""
        self.logger.error(f"[BROWSER] âŒ æµè§ˆå™¨å´©æºƒ: {error_msg}")
        self.logger.info("[BROWSER] ğŸ”„ å‡†å¤‡é‡å¯æµè§ˆå™¨...")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡å¯
        recovery_delay = random.uniform(5, 10)
        self.logger.info(f"[RECOVERY] â±ï¸ æ¢å¤å»¶è¿Ÿ: {recovery_delay:.1f}ç§’")
        time.sleep(recovery_delay)
        
        return True  # è¡¨ç¤ºéœ€è¦é‡æ–°åˆ›å»ºæµè§ˆå™¨
    
    def _safe_delay_with_health_check(self, page, total_delay):
        """å®‰å…¨çš„åˆ†æ®µå»¶è¿Ÿ+æµè§ˆå™¨å¥åº·æ£€æŸ¥"""
        try:
            # å°†é•¿å»¶è¿Ÿåˆ†æˆå¤šä¸ªçŸ­å»¶è¿Ÿæ®µ
            segment_duration = 3  # æ¯æ®µ3ç§’
            segments = int(total_delay / segment_duration)
            remaining = total_delay % segment_duration
            
            self.logger.debug(f"[DELAY] åˆ†æ®µå»¶è¿Ÿ: {segments}æ®µÃ—{segment_duration}ç§’ + {remaining:.1f}ç§’")
            
            # æ‰§è¡Œåˆ†æ®µå»¶è¿Ÿ
            for i in range(segments):
                time.sleep(segment_duration)
                
                # æ¯æ®µåæ£€æŸ¥æµè§ˆå™¨çŠ¶æ€
                if not self._is_browser_alive(page):
                    self.logger.error("[DELAY] âŒ å»¶è¿ŸæœŸé—´æµè§ˆå™¨æ–­å¼€")
                    raise Exception("Browser disconnected during delay")
                
                # æ˜¾ç¤ºè¿›åº¦
                elapsed = (i + 1) * segment_duration
                self.logger.debug(f"[DELAY] å»¶è¿Ÿè¿›åº¦: {elapsed:.1f}/{total_delay:.1f}ç§’")
            
            # å‰©ä½™æ—¶é—´
            if remaining > 0:
                time.sleep(remaining)
                if not self._is_browser_alive(page):
                    self.logger.error("[DELAY] âŒ å»¶è¿ŸæœŸé—´æµè§ˆå™¨æ–­å¼€")
                    raise Exception("Browser disconnected during delay")
                    
            self.logger.debug("[DELAY] âœ… å»¶è¿Ÿå®Œæˆï¼Œæµè§ˆå™¨çŠ¶æ€æ­£å¸¸")
            
        except Exception as e:
            self.logger.error(f"[DELAY] âŒ å»¶è¿ŸæœŸé—´å‡ºç°å¼‚å¸¸: {e}")
            raise e
    
    def simulate_intelligent_behavior(self, page, behavior_config):
        """æ™ºèƒ½ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ - å¢åŠ æµè§ˆå™¨å´©æºƒæ£€æµ‹"""
        try:
            # é¦–å…ˆæ£€æŸ¥æµè§ˆå™¨çŠ¶æ€
            if not self._is_browser_alive(page):
                self.logger.warning("[BEHAVIOR] âš ï¸ æµè§ˆå™¨å·²æ–­å¼€ï¼Œè·³è¿‡ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ")
                raise Exception("Browser disconnected")
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ»šåŠ¨
            if behavior_config['should_scroll']:
                try:
                    scroll_distance = random.randint(300, 800)
                    self._safe_evaluate(page, f'window.scrollBy(0, {scroll_distance})', timeout=3000)
                    scroll_delay = AntiDetectionConfig.get_random_delay('request_delay')
                    time.sleep(scroll_delay)
                    self.logger.debug("[BEHAVIOR] ğŸ“œ æ‰§è¡Œæ»šåŠ¨è¡Œä¸º")
                except Exception as e:
                    self.logger.warning(f"[BEHAVIOR] æ»šåŠ¨å¤±è´¥: {e}")
                
            # ç§»é™¤ç‚¹å‡»æ“ä½œ - è¿™æ˜¯å¯¼è‡´é—®é¢˜çš„ä¸»è¦åŸå› ä¹‹ä¸€
            # åŸæ¥çš„ç‚¹å‡»ä»£ç è¢«ç§»é™¤
                    
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ‚¬åœ - ç®€åŒ–æ‚¬åœæ“ä½œ
            if behavior_config.get('should_hover', False) and random.random() < 0.3:  # é™ä½æ‚¬åœæ¦‚ç‡
                try:
                    if self._is_browser_alive(page):
                        hover_elements = page.query_selector_all('div[class*="shop"]', timeout=3000)
                        if hover_elements and len(hover_elements) > 0:
                            element = random.choice(hover_elements[:3])  # åªé€‰æ‹©å‰3ä¸ªå…ƒç´ 
                            element.hover(timeout=2000)
                            hover_delay = min(AntiDetectionConfig.get_random_delay('request_delay'), 2)  # é™åˆ¶æœ€å¤§å»¶è¿Ÿ
                            time.sleep(hover_delay)
                            self.logger.debug("[BEHAVIOR] ğŸ–±ï¸ æ‰§è¡Œæ‚¬åœè¡Œä¸º")
                except Exception as e:
                    self.logger.debug(f"[BEHAVIOR] æ‚¬åœå¤±è´¥: {e}")
                    
            # æ ¹æ®é…ç½®çš„åœç•™æ¨¡å¼å†³å®šåœç•™æ—¶é—´ - ç¼©çŸ­åœç•™æ—¶é—´
            try:
                patterns = AntiDetectionConfig.get_behavior_patterns()
                stay_pattern = behavior_config['stay_pattern']
                min_time, max_time = patterns['stay_patterns'][stay_pattern]
                # ç¼©çŸ­åœç•™æ—¶é—´åˆ°åŸæ¥çš„ä¸€åŠ
                stay_time = random.uniform(min_time * 0.5, max_time * 0.5)
                time.sleep(stay_time)
                self.logger.debug(f"[BEHAVIOR] â±ï¸ {stay_pattern}åœç•™: {stay_time:.1f}ç§’")
            except Exception as e:
                self.logger.debug(f"[BEHAVIOR] åœç•™æ—¶é—´è®¡ç®—å¤±è´¥: {e}")
                time.sleep(random.uniform(1, 3))  # é»˜è®¤åœç•™æ—¶é—´
            
        except Exception as e:
            self.logger.warning(f"[BEHAVIOR] âš ï¸ æ™ºèƒ½è¡Œä¸ºæ¨¡æ‹Ÿå¤±è´¥: {e}")
            # å¦‚æœæ˜¯æµè§ˆå™¨æ–­å¼€ï¼Œé‡æ–°æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
            if "Browser disconnected" in str(e) or "Target page, context or browser has been closed" in str(e):
                raise e

    def crawl_specific_task(self, city_name, category_names, start_page=1, end_page=15, sort_type='popularity'):
        """
        æ‰§è¡Œç‰¹å®šçˆ¬å–ä»»åŠ¡ - æ”¯æŒé¡µæ•°èŒƒå›´å’Œæ’åº
        Args:
            city_name: åŸå¸‚ä¸­æ–‡åç§°ï¼Œå¦‚ 'è¥¿å®‰'
            category_names: å“ç±»ä¸­æ–‡åç§°åˆ—è¡¨ï¼Œå¦‚ ['å’–å•¡', 'é¥®å“']
            start_page: èµ·å§‹é¡µæ•°ï¼ˆé»˜è®¤1ï¼‰
            end_page: ç»“æŸé¡µæ•°ï¼ˆé»˜è®¤15ï¼‰
            sort_type: æ’åºæ–¹å¼ï¼Œ'popularity'(äººæ°”æœ€å¤š) æˆ– 'reviews'(è¯„ä»·æœ€å¤š)
        """
        # è·å–åŸå¸‚ä»£ç 
        if city_name not in self.cities:
            self._update_status(f"ä¸æ”¯æŒçš„åŸå¸‚: {city_name}", status_type='error')
            return False, []
        
        city_code = self.cities[city_name]
        
        # è·å–å“ç±»ID
        category_ids = []
        for category_name in category_names:
            if category_name not in self.categories:
                self._update_status(f"ä¸æ”¯æŒçš„å“ç±»: {category_name}", status_type='error')
                return False, []
            category_ids.append(self.categories[category_name])
        
        # è°ƒç”¨å†…éƒ¨å®ç°
        return self._crawl_specific_task_internal(city_code, city_name, category_ids, category_names, start_page, end_page, sort_type)
    
    def _crawl_specific_task_internal(self, city_code, city_name, category_ids, category_names, start_page=1, end_page=15, sort_type='popularity'):
        """
        å†…éƒ¨çˆ¬å–å®ç°æ–¹æ³• - å¢å¼ºç‰ˆï¼Œç±»ä¼¼é‡æ„/custom_crawler_for_specific_task.py
        Args:
            city_code: åŸå¸‚ä»£ç ï¼Œå¦‚ 'xian'
            city_name: åŸå¸‚ä¸­æ–‡åï¼Œå¦‚ 'è¥¿å®‰'
            category_ids: å“ç±»IDåˆ—è¡¨ï¼Œå¦‚ ['g132', 'g34236']
            category_names: å“ç±»ä¸­æ–‡ååˆ—è¡¨ï¼Œå¦‚ ['å’–å•¡', 'é¥®å“']
            start_page: èµ·å§‹é¡µæ•°
            end_page: ç»“æŸé¡µæ•°
            sort_type: æ’åºæ–¹å¼ï¼Œ'popularity'(äººæ°”æœ€å¤š) æˆ– 'reviews'(è¯„ä»·æœ€å¤š)
        """
        
        self.logger.info("=" * 60)
        self.logger.info(f"[TASK] ğŸš€ å¼€å§‹çˆ¬å–ä»»åŠ¡")
        self.logger.info(f"[TASK] ğŸ“ åŸå¸‚: {city_name} ({city_code})")
        self.logger.info(f"[TASK] ğŸ“‚ å“ç±»: {category_names} ({len(category_names)}ä¸ª)")
        self.logger.info(f"[TASK] ğŸ“„ é¡µæ•°èŒƒå›´: {start_page}-{end_page}é¡µ")
        self.logger.info("=" * 60)
        
        self._update_status(f"ğŸš€ å¼€å§‹çˆ¬å–ä»»åŠ¡: {city_name} - {', '.join(category_names)} ({start_page}-{end_page}é¡µ)")
        
        # é‡ç½®ç»Ÿè®¡å˜é‡
        self.captcha_count = 0
        self.skipped_pages = 0
        self.page_refresh_count = 0
        self.ua_change_count = 0

        if not self.cookie_string:
            self.logger.error("[TASK] âŒ Cookieä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œçˆ¬å–ä»»åŠ¡")
            self._update_status("âŒ Cookieä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œçˆ¬å–ä»»åŠ¡", status_type='error')
            return False, []
        
        task_start_time = datetime.now()
        all_task_data = []
        
        with sync_playwright() as p:
            try:
                self.logger.info("[BROWSER] ğŸŒ åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡...")
                browser, context = self.create_browser_context(p)
                
                # æ¸…ç†æµè§ˆå™¨æ•°æ®ä»¥é¿å…è®¾å¤‡å…³è”
                self.clear_browser_data(context)
                
                # ç§»é™¤åŠ¨æ€æŒ‡çº¹æ›´æ¢æœºåˆ¶ - è¿™å¯èƒ½å¯¼è‡´æµè§ˆå™¨ä¸ç¨³å®š
                # åŸæ¥çš„åŠ¨æ€æŒ‡çº¹æ›´æ¢ä»£ç è¢«ç§»é™¤
                
                # éšæœºå»¶è¿Ÿä»¥é¿å…æ—¶é—´æ¨¡å¼å…³è”
                initial_delay = AntiDetectionConfig.get_random_delay('initial_delay')
                self.logger.info(f"[PRIVACY] â±ï¸ åˆå§‹éšæœºå»¶è¿Ÿ: {initial_delay:.1f}ç§’")
                time.sleep(initial_delay)
                
                cookies = self.parse_cookies()
                context.add_cookies(cookies)
                self.logger.info(f"[COOKIE] âœ… å·²æ·»åŠ  {len(cookies)} ä¸ªCookie")
                
                page = context.new_page()
                fingerprint_script = self.get_browser_fingerprint_script()
                page.add_init_script(fingerprint_script)
                
                total_categories = len(category_names)
                saved_files = []
                
                for i, (category_id, category_name) in enumerate(zip(category_ids, category_names)):
                    category_start_time = datetime.now()
                    category_progress = (i / total_categories) * 100
                    
                    self.logger.info("-" * 50)
                    self.logger.info(f"[CATEGORY] ğŸ“‚ å¼€å§‹å¤„ç†å“ç±» {i+1}/{total_categories}: {category_name}")
                    self.logger.info(f"[CATEGORY] ğŸ†” å“ç±»ID: {category_id}")
                    self.logger.info(f"[CATEGORY] â±ï¸ å¼€å§‹æ—¶é—´: {category_start_time.strftime('%H:%M:%S')}")
                    
                    self._update_status(f"ğŸ“‚ å¼€å§‹å¤„ç†å“ç±» {i+1}/{total_categories}: {category_name}",
                                      progress=category_progress)
                     
                    category_data = []
                    consecutive_empty_pages = 0
                    page_range = end_page - start_page + 1
                    
                    # åŠ¨æ€è®¾ç½®è¿ç»­æ— æ•°æ®é˜ˆå€¼
                    if page_range <= 5:
                        max_consecutive_empty = page_range
                    else:
                        max_consecutive_empty = max(3, page_range // 2)
                    
                    self.logger.info(f"[CATEGORY] ğŸ“Š é¡µæ•°èŒƒå›´: {start_page}-{end_page}é¡µ")
                    self.logger.info(f"[CATEGORY] âš ï¸ æœ€å¤§è¿ç»­æ— æ•°æ®é¡µé¢: {max_consecutive_empty}")

                    # çˆ¬å–æŒ‡å®šé¡µæ•°
                    for page_num in range(start_page, end_page + 1):
                        page_start_time = datetime.now()
                        
                        # æ„å»ºURLï¼ˆæ·»åŠ æ’åºå‚æ•°ï¼‰
                        sort_options = {
                            'popularity': 'o2',     # äººæ°”æœ€å¤š
                            'reviews': 'o11',       # è¯„ä»·æœ€å¤š
                            'default': 'o2'         # é»˜è®¤æ’åºï¼ˆäººæ°”æœ€å¤šï¼‰
                        }
                        sort_suffix = sort_options.get(sort_type, sort_options['default'])

                        if page_num == 1:
                            url = f"https://www.dianping.com/{city_code}/ch10/{category_id}{sort_suffix}"
                        else:
                            url = f"https://www.dianping.com/{city_code}/ch10/{category_id}{sort_suffix}p{page_num}"
                        
                        self.logger.info(f"[PAGE] ğŸ“„ ç¬¬{page_num}é¡µ: {url}")
                        self.logger.info(f"[PAGE] â±ï¸ å¼€å§‹æ—¶é—´: {page_start_time.strftime('%H:%M:%S')}")
                        
                        page_progress = category_progress + ((page_num - start_page + 1) / page_range) * (100 / total_categories)
                        self._update_status(f"ğŸ“„ çˆ¬å–ç¬¬{page_num}é¡µ: {category_name}", progress=page_progress)
                        
                        try:
                            self.logger.info(f"[PAGE] ğŸ”„ æ­£åœ¨åŠ è½½é¡µé¢: {url}")
                            
                            # å¢å¼ºé¡µé¢åŠ è½½ç¨³å®šæ€§
                            max_retries = 3
                            for retry in range(max_retries):
                                try:
                                    page.goto(url, timeout=30000, wait_until='domcontentloaded')
                                    
                                    # ç­‰å¾…é¡µé¢ç¨³å®šå¹¶éªŒè¯åŠ è½½çŠ¶æ€
                                    page_delay = AntiDetectionConfig.get_random_delay('page_delay')
                                    time.sleep(page_delay)
                                    
                                    # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
                                    ready_state = page.evaluate('document.readyState')
                                    if ready_state == 'complete':
                                        self.logger.info(f"[PAGE] âœ… é¡µé¢åŠ è½½æˆåŠŸ: {url}")
                                        break
                                    else:
                                        self.logger.warning(f"[PAGE] âš ï¸ é¡µé¢æœªå®Œå…¨åŠ è½½ï¼ŒçŠ¶æ€: {ready_state}")
                                        if retry < max_retries - 1:
                                            retry_delay = AntiDetectionConfig.get_random_delay('error_delay')
                                            time.sleep(retry_delay)
                                            continue
                                        
                                except Exception as goto_error:
                                    self.logger.warning(f"[PAGE] âš ï¸ é¡µé¢åŠ è½½å°è¯• {retry + 1}/{max_retries} å¤±è´¥: {goto_error}")
                                    if retry < max_retries - 1:
                                        error_delay = AntiDetectionConfig.get_random_delay('error_delay')
                                        time.sleep(error_delay)
                                        continue
                                    else:
                                        raise goto_error
                            
                            # é¢å¤–çš„é¡µé¢ç¨³å®šæ€§æ£€æŸ¥
                            stability_delay = AntiDetectionConfig.get_random_delay('request_delay')
                            time.sleep(stability_delay)
                            
                            # æ£€æŸ¥éªŒè¯ç 
                            captcha = self.detect_captcha(page)
                            if captcha:
                                self.captcha_count += 1
                                self.logger.warning(f"[CAPTCHA] ğŸš¨ æ£€æµ‹åˆ°éªŒè¯ç ï¼ç¬¬{page_num}é¡µ - {category_name}")
                                self.logger.warning(f"[CAPTCHA] ğŸ” è¯¦ç»†ä¿¡æ¯: {captcha}")
                                
                                self._update_status(f"ğŸš¨ æ£€æµ‹åˆ°éªŒè¯ç ï¼ç¬¬{page_num}é¡µ - {category_name}", status_type='warning')
                                self._update_status("â³ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ç éªŒè¯...", status_type='warning')
                                
                                # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨è§£å†³éªŒè¯ç 
                                max_wait_time = 300
                                wait_interval = 10
                                waited_time = 0
                                
                                while waited_time < max_wait_time:
                                    time.sleep(wait_interval)
                                    waited_time += wait_interval
                                    
                                    current_captcha = self.detect_captcha(page)
                                    if not current_captcha:
                                        self.logger.info("[CAPTCHA] âœ… éªŒè¯ç å·²è§£å†³")
                                        self._update_status("âœ… éªŒè¯ç å·²è§£å†³ï¼Œç»§ç»­çˆ¬å–...", status_type='success')
                                        break
                                    else:
                                        remaining_time = max_wait_time - waited_time
                                        if remaining_time > 0:
                                            self.logger.info(f"[CAPTCHA] â±ï¸ ç­‰å¾…ä¸­...å‰©ä½™{remaining_time}ç§’")
                                            self._update_status(f"â±ï¸ ç­‰å¾…éªŒè¯ç è§£å†³ä¸­...å‰©ä½™{remaining_time}ç§’", status_type='info')
                                
                                if waited_time >= max_wait_time:
                                    self.skipped_pages += 1
                                    self.logger.warning(f"[CAPTCHA] âš ï¸ éªŒè¯ç ç­‰å¾…è¶…æ—¶ï¼Œè·³è¿‡ç¬¬{page_num}é¡µ")
                                    self._update_status("âš ï¸ éªŒè¯ç ç­‰å¾…è¶…æ—¶ï¼Œè·³è¿‡å½“å‰é¡µé¢", status_type='warning')
                                    continue
                                
                                # éªŒè¯ç è§£å†³åé‡æ–°åŠ è½½
                                try:
                                    page.reload(timeout=30000)
                                    reload_delay = AntiDetectionConfig.get_random_delay('page_delay')
                                    time.sleep(reload_delay)
                                    self.logger.info("[PAGE] ğŸ”„ é¡µé¢é‡æ–°åŠ è½½å®Œæˆ")
                                except Exception as e:
                                    self.logger.error(f"[PAGE] âŒ é¡µé¢åˆ·æ–°å¤±è´¥: {e}")
                                    continue
                             
                            # æ™ºèƒ½User-Agentè½®æ¢
                            ua_config = AntiDetectionConfig.get_user_agents()
                            if random.random() < ua_config['change_frequency']:
                                try:
                                    new_ua = self.get_random_user_agent()
                                    page.set_extra_http_headers({'User-Agent': new_ua})
                                    self.ua_change_count += 1
                                    self.logger.info(f"[UA] ğŸ”„ ç¬¬{page_num}é¡µæ™ºèƒ½æ›´æ¢User-Agent")
                                except Exception as e:
                                    self.logger.warning(f"[UA] âš ï¸ User-Agentæ›´æ¢å¤±è´¥: {e}")

                            # æ™ºèƒ½ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ
                            behavior = AntiDetectionConfig.get_random_behavior()
                            self.simulate_intelligent_behavior(page, behavior)
                             
                            # æå–æ•°æ®
                            page_shops = self.extract_shop_data(page, city_name, category_name)
                            page_end_time = datetime.now()
                            page_duration = (page_end_time - page_start_time).total_seconds()

                            if page_shops:
                                category_data.extend(page_shops)
                                consecutive_empty_pages = 0
                                self.logger.info(f"[PAGE] âœ… ç¬¬{page_num}é¡µæˆåŠŸ: {len(page_shops)} ä¸ªå•†é“º (è€—æ—¶{page_duration:.1f}ç§’)")
                                self._update_status(f"âœ… ç¬¬{page_num}é¡µæˆåŠŸ: {len(page_shops)} ä¸ªå•†é“º")
                            else:
                                consecutive_empty_pages += 1
                                self.logger.warning(f"[PAGE] âš ï¸ ç¬¬{page_num}é¡µæ— æ•°æ® (è€—æ—¶{page_duration:.1f}ç§’)")
                                self._update_status(f"âš ï¸ ç¬¬{page_num}é¡µæ— æ•°æ®", status_type='warning')

                                if consecutive_empty_pages >= max_consecutive_empty:
                                    self.logger.warning(f"[CATEGORY] âš ï¸ è¿ç»­{consecutive_empty_pages}é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–å“ç±»: {category_name}")
                                    self._update_status(f"âš ï¸ è¿ç»­{consecutive_empty_pages}é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–å“ç±»: {category_name}",
                                                      status_type='warning')
                                    break
                             
                            # é¡µé¢é—´å»¶è¿Ÿ - ä¼˜åŒ–ä¸ºåˆ†æ®µå»¶è¿Ÿ+å¥åº·æ£€æŸ¥
                            if page_num < end_page:
                                # ç¼©çŸ­åŸºç¡€å»¶è¿Ÿæ—¶é—´
                                base_delay = random.uniform(8, 15)  # ä»20-35ç¼©çŸ­åˆ°8-15
                                
                                if self.captcha_count > 0 and (page_num - 1) % 5 == 0:
                                    base_delay += random.uniform(5, 10)  # ä»10-20ç¼©çŸ­åˆ°5-10
                                
                                if page_num % 10 == 0:
                                    base_delay += random.uniform(8, 15)  # ä»15-30ç¼©çŸ­åˆ°8-15
                                
                                delay_pattern = random.choice(['normal', 'careful', 'relaxed'])
                                if delay_pattern == 'careful':
                                    base_delay *= random.uniform(1.1, 1.3)  # ä»1.2-1.5ç¼©å°åˆ°1.1-1.3
                                elif delay_pattern == 'relaxed':
                                    base_delay *= random.uniform(0.8, 1.0)
                                
                                self.logger.info(f"[DELAY] â±ï¸ é¡µé¢å»¶è¿Ÿ({delay_pattern}): {base_delay:.1f}ç§’")
                                self._update_status(f"â±ï¸ é¡µé¢å»¶è¿Ÿ({delay_pattern}): {base_delay:.1f}ç§’")
                                
                                # åˆ†æ®µå»¶è¿Ÿ+æµè§ˆå™¨å¥åº·æ£€æŸ¥
                                self._safe_delay_with_health_check(page, base_delay)
                             
                        except Exception as e:
                            self.logger.error(f"[PAGE] âŒ ç¬¬{page_num}é¡µå¼‚å¸¸: {e}")
                            self.logger.error(f"[PAGE] ğŸ” å¼‚å¸¸ç±»å‹: {type(e).__name__}")
                            self._update_status(f"âŒ ç¬¬{page_num}é¡µå¼‚å¸¸: {e}", status_type='error')
                            
                            # å°è¯•é¡µé¢æ¢å¤
                            try:
                                self.logger.info(f"[PAGE] ğŸ”„ å°è¯•æ¢å¤é¡µé¢çŠ¶æ€...")
                                page.wait_for_timeout(3000)  # ç­‰å¾…3ç§’
                                
                                # æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜å¯ç”¨
                                page.evaluate('document.readyState')
                                self.logger.info(f"[PAGE] âœ… é¡µé¢çŠ¶æ€æ­£å¸¸ï¼Œç»§ç»­ä¸‹ä¸€é¡µ")
                                continue
                            except Exception as recovery_error:
                                self.logger.error(f"[PAGE] âŒ é¡µé¢æ¢å¤å¤±è´¥: {recovery_error}")
                                self.logger.warning(f"[PAGE] â­ï¸ è·³è¿‡ç¬¬{page_num}é¡µï¼Œç»§ç»­ä¸‹ä¸€é¡µ")
                                self._update_status(f"â­ï¸ é¡µé¢æ¢å¤å¤±è´¥ï¼Œè·³è¿‡ç¬¬{page_num}é¡µ", status_type='warning')
                                continue
                     
                    category_end_time = datetime.now()
                    category_duration = (category_end_time - category_start_time).total_seconds()
                    
                    self.logger.info("-" * 40)
                    self.logger.info(f"[CATEGORY] âœ… å“ç±» {category_name} å®Œæˆ")
                    self.logger.info(f"[CATEGORY] ğŸ“Š å•†é“ºæ•°: {len(category_data)} ä¸ª")
                    self.logger.info(f"[CATEGORY] â±ï¸ è€—æ—¶: {category_duration:.1f}ç§’")
                    
                    all_task_data.extend(category_data)
                    
                    # æ¯å®Œæˆä¸€ä¸ªå“ç±»å°±ä¿å­˜æ•°æ®ï¼ˆå¢é‡ä¿å­˜ï¼‰
                    if category_data:
                        import sys
                        import os
                        sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
                        from config.crawler_config import FILE_PATHS
                        
                        save_result = self.save_task_data(
                            category_data,
                            city_name,
                            [category_name],  # åªä¿å­˜å½“å‰å“ç±»
                            FILE_PATHS['OUTPUTS_DIR'],
                            incremental=True,
                            category_name=category_name
                        )
                        if save_result:
                            saved_files.append(save_result['filename'])
                    
                    # å“ç±»é—´å»¶è¿Ÿ
                    if i < len(category_names) - 1:
                        delay = AntiDetectionConfig.get_random_delay('category_delay')
                        self.logger.info(f"[DELAY] â±ï¸ å“ç±»é—´å»¶è¿Ÿ: {delay:.1f}ç§’")
                        self._update_status(f"â±ï¸ å“ç±»é—´å»¶è¿Ÿ: {delay:.1f}ç§’")
                        time.sleep(delay)
                
                # ä»»åŠ¡å®Œæˆç»Ÿè®¡
                task_end_time = datetime.now()
                task_duration = (task_end_time - task_start_time).total_seconds()
                
                self.logger.info("=" * 60)
                self.logger.info("[TASK] ğŸ‰ ä»»åŠ¡å®Œæˆ!")
                self.logger.info(f"[TASK] â±ï¸ æ€»è€—æ—¶: {task_duration/60:.1f}åˆ†é’Ÿ")
                self.logger.info(f"[TASK] ğŸª æ€»å•†é“º: {len(all_task_data)} ä¸ª")
                self.logger.info(f"[TASK] ğŸ“‚ å“ç±»æ•°: {len(category_names)} ä¸ª")
                self.logger.info("[TASK] ğŸ›¡ï¸ åæ£€æµ‹ç»Ÿè®¡:")
                self.logger.info(f"[TASK]   éªŒè¯ç é‡åˆ°: {self.captcha_count} æ¬¡")
                self.logger.info(f"[TASK]   è·³è¿‡é¡µé¢: {self.skipped_pages} é¡µ")
                self.logger.info(f"[TASK]   é¡µé¢åˆ·æ–°: {self.page_refresh_count} æ¬¡")
                self.logger.info(f"[TASK]   UAæ›´æ¢: {self.ua_change_count} æ¬¡")
                
                if saved_files:
                    self.logger.info("[TASK] ğŸ’¾ å·²ä¿å­˜æ–‡ä»¶:")
                    for file in saved_files:
                        self.logger.info(f"[TASK]   ğŸ“ {file}")
                
                self.logger.info("=" * 60)
                
                # åˆå¹¶æ‰€æœ‰å¢é‡æ–‡ä»¶ä¸ºæœ€ç»ˆæ–‡ä»¶
                if len(category_names) > 1 and all_task_data:
                    import sys
                    import os
                    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
                    from config.crawler_config import FILE_PATHS
                    
                    final_save = self.save_task_data(
                        all_task_data,
                        city_name,
                        category_names,
                        FILE_PATHS['OUTPUTS_DIR']
                    )
                    if final_save:
                        saved_files.append(final_save['filename'])
                
                self._update_status(f"ğŸ‰ ä»»åŠ¡å®Œæˆ! æ€»è€—æ—¶: {task_duration/60:.1f}åˆ†é’Ÿï¼Œæ€»å•†é“º: {len(all_task_data)} ä¸ª",
                                  progress=100, status_type='success')
                
                return True, all_task_data, saved_files
                
            except Exception as e:
                self.logger.error(f"[TASK] âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}", exc_info=True)
                self._update_status(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}", status_type='error')
                return False, []
            
            finally:
                # å¢å¼ºçš„æµè§ˆå™¨èµ„æºå®‰å…¨å…³é—­é€»è¾‘
                self.logger.info("[BROWSER] ğŸ”’ å¼€å§‹å®‰å…¨å…³é—­æµè§ˆå™¨èµ„æº...")
                
                # 1. å…³é—­é¡µé¢
                try:
                    if 'page' in locals() and page and not page.is_closed():
                        self.logger.info("[BROWSER] ğŸ”’ æ­£åœ¨å…³é—­é¡µé¢...")
                        page.close()
                        self.logger.info("[BROWSER] âœ… é¡µé¢å·²å…³é—­")
                    elif 'page' in locals() and page:
                        self.logger.info("[BROWSER] â„¹ï¸ é¡µé¢å·²ç»å…³é—­")
                except Exception as e:
                    self.logger.warning(f"[BROWSER] âš ï¸ é¡µé¢å…³é—­å¼‚å¸¸: {e}")
                    try:
                        # å¼ºåˆ¶å…³é—­é¡µé¢
                        if 'page' in locals() and page:
                            page._impl_obj._connection.send('Page.close', {})
                    except:
                        pass
                
                # ç­‰å¾…é¡µé¢èµ„æºé‡Šæ”¾
                time.sleep(1)
                
                # 2. å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡
                try:
                    if 'context' in locals() and context and not context._impl_obj._is_closed_or_closing:
                        self.logger.info("[BROWSER] ğŸ”’ æ­£åœ¨å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡...")
                        context.close()
                        self.logger.info("[BROWSER] âœ… æµè§ˆå™¨ä¸Šä¸‹æ–‡å·²å…³é—­")
                    elif 'context' in locals() and context:
                        self.logger.info("[BROWSER] â„¹ï¸ æµè§ˆå™¨ä¸Šä¸‹æ–‡å·²ç»å…³é—­")
                except Exception as e:
                    self.logger.warning(f"[BROWSER] âš ï¸ æµè§ˆå™¨ä¸Šä¸‹æ–‡å…³é—­å¼‚å¸¸: {e}")
                
                # ç­‰å¾…ä¸Šä¸‹æ–‡èµ„æºé‡Šæ”¾
                time.sleep(1)
                
                # 3. å…³é—­æµè§ˆå™¨
                try:
                    if 'browser' in locals() and browser and browser.is_connected():
                        self.logger.info("[BROWSER] ğŸ”’ æ­£åœ¨å…³é—­æµè§ˆå™¨...")
                        browser.close()
                        self.logger.info("[BROWSER] âœ… æµè§ˆå™¨å·²å…³é—­")
                    elif 'browser' in locals() and browser:
                        self.logger.info("[BROWSER] â„¹ï¸ æµè§ˆå™¨å·²ç»å…³é—­")
                except Exception as e:
                    self.logger.warning(f"[BROWSER] âš ï¸ æµè§ˆå™¨å…³é—­å¼‚å¸¸: {e}")
                    try:
                        # å°è¯•å¼ºåˆ¶ç»ˆæ­¢æµè§ˆå™¨è¿›ç¨‹
                        if 'browser' in locals() and browser:
                            browser._impl_obj._connection.send('Browser.close', {})
                    except:
                        pass
                
                # æœ€ç»ˆç­‰å¾…ç¡®ä¿æ‰€æœ‰èµ„æºå®Œå…¨é‡Šæ”¾
                time.sleep(2)
                self.logger.info("[BROWSER] ğŸ”’ æ‰€æœ‰æµè§ˆå™¨èµ„æºå·²å®‰å…¨é‡Šæ”¾")

    def save_task_data(self, data, city_name, category_names, output_dir, incremental=False, category_name=None):
        """ä¿å­˜ä»»åŠ¡æ•°æ®åˆ°æŒ‡å®šç›®å½•ï¼Œæ”¯æŒå¢é‡ä¿å­˜"""
        if not data and not incremental:
            self.logger.warning("[SAVE] âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            self._update_status("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜", status_type='warning')
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if incremental and category_name:
            # å¢é‡ä¿å­˜å•ä¸ªå“ç±»
            filename = f'custom_crawl_{city_name}_{category_name}_partial_{timestamp}.csv'
            display_name = f"{city_name}_{category_name}_éƒ¨åˆ†æ•°æ®"
        else:
            # å®Œæ•´ä¿å­˜
            categories_str = "_".join(category_names)
            filename = f'custom_crawl_{city_name}_{categories_str}_{timestamp}.csv'
            display_name = f"{city_name}_{categories_str}"
        
        filepath = os.path.join(output_dir, filename)
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¿½åŠ ï¼Œå¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶
            file_exists = os.path.exists(filepath)
            write_header = not file_exists or not incremental
            
            with open(filepath, 'a' if incremental else 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=self.core_fields)
                if write_header:
                    writer.writeheader()
                for shop in data:
                    writer.writerow(shop)
            
            self.logger.info(f"[SAVE] âœ… æ•°æ®å·²{'è¿½åŠ åˆ°' if incremental else 'ä¿å­˜åˆ°'}: {filename}")
            self._update_status(f"âœ… æ•°æ®å·²ä¿å­˜: {display_name} ({len(data)}ä¸ªå•†é“º)")
            
            # æ•°æ®è´¨é‡åˆ†æ
            price_complete = sum(1 for shop in data if shop['avg_price'])
            price_rate = (price_complete / len(data)) * 100 if data else 0
            
            # æŒ‰å“ç±»ç»Ÿè®¡
            category_stats = {}
            for shop in data:
                cat = shop['secondary_category']
                if cat not in category_stats:
                    category_stats[cat] = 0
                category_stats[cat] += 1
            
            # è®°å½•ä¿å­˜ä¿¡æ¯
            self.logger.info("[SAVE] ğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
            self.logger.info(f"[SAVE]   æ€»å•†é“ºæ•°: {len(data)}")
            self.logger.info(f"[SAVE]   ä»·æ ¼å®Œæ•´ç‡: {price_rate:.1f}% ({price_complete}/{len(data)})")
            self.logger.info("[SAVE]   å“ç±»åˆ†å¸ƒ:")
            for cat, count in category_stats.items():
                self.logger.info(f"[SAVE]     {cat}: {count} ä¸ªå•†é“º")
            
            return {
                'filename': filename,
                'filepath': filepath,
                'total_shops': len(data),
                'price_complete_rate': price_rate,
                'category_stats': category_stats,
                'is_incremental': incremental
            }
            
        except Exception as e:
            self.logger.error(f"[SAVE] âŒ ä¿å­˜å¤±è´¥: {e}", exc_info=True)
            self._update_status(f"âŒ ä¿å­˜å¤±è´¥: {e}", status_type='error')
            return None